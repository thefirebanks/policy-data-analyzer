{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish preprocessor/sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dafirebanks/Projects/policy-data-analyzer'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.text_preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup amazon client\n",
    "language = \"spanish\"\n",
    "bucket_name = \"wri-nlp-policy\"\n",
    "creds_filepath = \"/Users/dafirebanks/Documents/credentials.json\"\n",
    "\n",
    "s3_client = S3Client(creds_filepath=creds_filepath, bucket_name=bucket_name, language=language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File_id: /000bd482c20f22b801193f1897ed1eca89318a53\n",
      "Text: SECRETARIA DE DESARROLLO URBANO Y OBRAS\n",
      "PUBLICAS DEL ESTADO DE QUERETARO\n",
      "DEPARTAMENTO DE CONCURSOS\n",
      "R\n",
      "=======================================\n",
      "File_id: /000d91e74dd312e2f8d2f28bd154c3cab6a8bc10\n",
      "Text: \n",
      "  CONVENIO de Coordinación en materia de reasignación de recursos que celebran la Secretaría de Com\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "# 2. Make sure we're getting the right files\n",
    "i = 0\n",
    "for file_id, text in s3_client.load_text_files(language):\n",
    "    print(\"File_id:\", file_id)\n",
    "    print(\"Text:\", text[:100])\n",
    "    print(\"=======================================\")\n",
    "    i += 1\n",
    "    if i == 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Start preprocessing\n",
    "tokenizer = nltk.data.load(f\"tokenizers/punkt/{language}.pickle\")\n",
    "abbrevs = {\"ord\", \"num\", \"sra\", \"no\", \"corp\", \"art\", \"ltda\", \"ud\"}\n",
    "min_num_words = 5\n",
    "\n",
    "new_text_files_folder = f\"{language}_documents/text_files/new\"\n",
    "processed_text_files_folder = f\"{language}_documents/text_files/processed\"\n",
    "\n",
    "i = 0\n",
    "print_every = 100\n",
    "error_files = []\n",
    "\n",
    "for file_id, text in s3_client.load_text_files(language):\n",
    "    try:\n",
    "        file_id = file_id.replace(\"/\", \"\")\n",
    "        preprocessed_text = preprocess_spanish_text(text)\n",
    "        sents = get_nltk_sents(preprocessed_text, tokenizer, abbrevs)\n",
    "        postprocessed_sents = format_sents_for_output(remove_short_sents(sents, min_num_words), file_id)\n",
    "        s3_client.store_sentences(postprocessed_sents, file_id, language)\n",
    "        s3_client.move_object(file_id + \".txt\", new_text_files_folder, processed_text_files_folder)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_files.append({file_id: e})\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    # For testing and early stopping, uncomment this\n",
    "#     if i == 2:\n",
    "#         break\n",
    "\n",
    "    if i % print_every == 0:\n",
    "        print(\"----------------------------------------------\")\n",
    "        print(f\"Processing {i} documents...\")\n",
    "        print(f\"Number of errors so far: {len(error_files)}\")\n",
    "        print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to store the errors\n",
    "with open(f\"../output/{language}_sentence_splitting_errors.json\", \"w\") as f:\n",
    "    json.dump(error_files, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code\n",
    "\n",
    "From this section until the end, it's mainly experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import nltk.data\n",
    "import spacy \n",
    "import string\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    return re.sub(re.compile('<.*?>'), '', text)\n",
    "\n",
    "def replace_links(text):\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "    return re.sub(r'www\\S+', '[URL]', text)\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "def parse_emails(text):\n",
    "    \"\"\" \n",
    "    Remove the periods from emails in text, except the last one\n",
    "    \"\"\"\n",
    "    emails = [email if email[-1] != \".\" else email[:-1] for email in re.findall(r\"\\S*@\\S*\\s?\", text)]\n",
    "    \n",
    "    for email in emails:\n",
    "        new_email = email.replace(\".\", \"\")\n",
    "        text = text.replace(email, new_email)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def parse_acronyms(text):\n",
    "    \"\"\" \n",
    "    Remove the periods from acronyms in the text (i.e \"U.S.\" becomes \"US\") \n",
    "    \"\"\"\n",
    "\n",
    "    acronyms = re.findall(r\"\\b(?:[a-zA-Z]\\.){2,}\", text)\n",
    "         \n",
    "    for acronym in acronyms:\n",
    "        new_acronym = acronym.replace(\".\", \"\")\n",
    "        text = text.replace(acronym, new_acronym)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def spanish_preprocessing(txt, remove_new_lines=False):\n",
    "    \"\"\"\n",
    "    Steps in the preprocessing of text:\n",
    "        1. Remove HTML tags\n",
    "        2. Replace URLS by a tag [URL]\n",
    "        3. Replace new lines and tabs by normal spaces - sometimes sentences have new lines in the middle\n",
    "        4. Remove excessive spaces (more than 1 occurrence)\n",
    "        5. Parse abreviations and acronyms\n",
    "    \"\"\"\n",
    "    txt = replace_links(remove_html_tags(txt)).strip()\n",
    "    if remove_new_lines:\n",
    "        txt = txt.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "    txt = remove_multiple_spaces(txt)\n",
    "    txt = parse_emails(txt)\n",
    "    txt = parse_acronyms(txt)\n",
    "    \n",
    "    new_txt = \"\"\n",
    "    all_period_idx = set([indices.start() for indices in re.finditer(\"\\.\", txt)])\n",
    "    \n",
    "    for i, char in enumerate(txt):\n",
    "        if i in all_period_idx:\n",
    "            # Any char following a period that is NOT a space means that we should not add that period\n",
    "            if i + 1 < len(txt) and txt[i + 1] != \" \":\n",
    "                continue\n",
    "            \n",
    "            # Any char that is a number following a period will not count. \n",
    "            # For enumerations, we're counting on docs being enumerated as \"(a)\" or \"(ii)\", and if not, they will be separated by the . after the number (\"3. Something\" will just be \"Something\" as a sentence)\n",
    "            if i + 2 < len(txt) and txt[i + 2].isnumeric(): \n",
    "                continue\n",
    "            \n",
    "            # If we wanted to have all numbered lists together, uncomment this, and comment out the previous condition\n",
    "#             if i + 2 < len(txt) and not txt[i + 2].isalpha(): \n",
    "#                 continue\n",
    "            \n",
    "        new_txt += char\n",
    "\n",
    "    return unidecode.unidecode(new_txt)\n",
    "\n",
    "def get_nltk_sents(txt, tokenizer, extra_abbreviations=None):\n",
    "    if extra_abbreviations:\n",
    "        tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "        \n",
    "    sents = tokenizer.tokenize(txt)\n",
    "    return sents\n",
    "\n",
    "def spanish_postprocessing(sents, min_num_words=4):\n",
    "    \"\"\"\n",
    "    Remove sentences that are made of less than a given number of words. Default is 4\n",
    "    \"\"\"\n",
    "    \n",
    "    return [sent for sent in sents if len(sent.split()) >= min_num_words]\n",
    "\n",
    "def format_sents_for_output(sents, doc_id):\n",
    "    formatted_sents = {}\n",
    "\n",
    "    for i, sent in enumerate(sents):\n",
    "        formatted_sents.update({f\"{doc_id}_sent_{i}\": {\"text\": sent, \"label\": []}})\n",
    "\n",
    "    return formatted_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "\n",
    "USC_re = re.compile('[Uu]\\.*[Ss]\\.*[Cc]\\.]+')\n",
    "PAREN_re = re.compile('\\([^(]+\\ [^\\(]+\\)')\n",
    "BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('\"#%&\\*\\+/<=>@[\\]^{|}~_'), re.UNICODE)\n",
    "BULLET_RE = re.compile('\\n[\\ \\t]*`*\\([a-zA-Z0-9]*\\)')\n",
    "DASH_RE = re.compile('--+')\n",
    "WHITESPACE_RE = re.compile('\\s+')\n",
    "EMPTY_SENT_RE = re.compile('[,\\.]\\ *[\\.,]')\n",
    "FIX_START_RE = re.compile('^[^A-Za-z]*')\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')\n",
    "\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "\n",
    "SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\\.|\\nSEC\\.* [0-9]{1,2}\\.|Sec\\.* [0-9]{1,2}\\.')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Borrowed from the FNDS text processing with additional logic added in.\n",
    "    Note: we do not take care of token breaking - assume SPACY's tokenizer\n",
    "    will handle this for us.\n",
    "    \"\"\"\n",
    "\n",
    "    # Indicate section headers, we need them for features\n",
    "    text = SECTION_HEADER_RE.sub('SECTION-HEADER', text)\n",
    "    # For simplicity later, remove '.' from most common acronym\n",
    "    text = text.replace(\"U.S.\", \"US\")\n",
    "    text = text.replace('SEC.', 'Section')\n",
    "    text = text.replace('Sec.', 'Section')\n",
    "    text = USC_re.sub('USC', text)\n",
    "\n",
    "    # Remove parantheticals because they are almost always references to laws \n",
    "    # We could add a special tag, but we just remove for now\n",
    "    # Note we dont get rid of nested parens because that is a complex re\n",
    "    #text = PAREN_re.sub('LAWREF', text)\n",
    "    text = PAREN_re.sub('', text)\n",
    "    \n",
    "\n",
    "    # Get rid of enums as bullets or ` as bullets\n",
    "    text = BULLET_RE.sub(' ',text)\n",
    "    \n",
    "    # Clean html \n",
    "    text = text.replace('&lt;all&gt;', '')\n",
    "\n",
    "    # Remove annoying punctuation, that's not relevant\n",
    "    text = BAD_PUNCT_RE.sub('', text)\n",
    "\n",
    "    # Get rid of long sequences of dashes - these are formating\n",
    "    text = DASH_RE.sub( ' ', text)\n",
    "\n",
    "    # removing newlines, tabs, and extra spaces.\n",
    "    text = WHITESPACE_RE.sub(' ', text)\n",
    "    \n",
    "    # If we ended up with \"empty\" sentences - get rid of them.\n",
    "    text = EMPTY_SENT_RE.sub('.', text)\n",
    "    \n",
    "    # Attempt to create sentences from bullets \n",
    "#    text = replace_semicolon(text)\n",
    "    \n",
    "    # Fix weird period issues + start of text weirdness\n",
    "    #text = re.sub('\\.(?=[A-Z])', '  . ', text)\n",
    "    # Get rid of anything thats not a word from the start of the text\n",
    "    text = FIX_START_RE.sub( '', text)\n",
    "    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   \n",
    "    text = FIX_PERIOD.sub(\". \\g<1>\", text)\n",
    "\n",
    "    # Fix quotes\n",
    "    text = text.replace('``', '\"')\n",
    "    text = text.replace('\\'\\'', '\"')\n",
    "\n",
    "    # Add special punct back in\n",
    "    text = text.replace('SECTION-HEADER', '<SECTION-HEADER>')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../input/Mexico/\"\n",
    "chile_paths = [\"Chile1.txt\", \"Chile2.txt\", \"Chile3.txt\"]\n",
    "elsalvador_paths = [\"ElSalvador1.txt\", \"ElSalvador2.txt\", \"ElSalvador3.txt\"]\n",
    "mexico_paths = [\"Mexico1.txt\", \"Mexico2.txt\", \"Mexico3.txt\", \"Mexico4.txt\", \"Mexico5.txt\", \"Mexico6.txt\"]\n",
    "fname = mexico_paths[1]\n",
    "txt_path = base_path + fname\n",
    "\n",
    "with open(txt_path, \"r\") as txt_file:\n",
    "    txt = txt_file.read()\n",
    "    \n",
    "# txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial out of the box result\n",
    "es_tokenizer = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")\n",
    "spa_abrevs = {\"ord\", \"num\", \"sra\", \"no\", \"corp\", \"art\", \"ltda\", \"ud\"}\n",
    "preprocessed1 = spanish_preprocessing(clean_text(txt))\n",
    "preprocessed2 = spanish_preprocessing(txt)\n",
    "s1 = get_nltk_sents(preprocessed1, es_tokenizer, spa_abrevs)\n",
    "s2 = get_nltk_sents(preprocessed2, es_tokenizer, spa_abrevs)\n",
    "len(s1), len(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "- We can add the `clean_text()` function but it may be more appropriate for english\n",
    "- With the execption of some weird characters, and some acronyms, sentences are parsed properly\n",
    "- Only concern is sometimes bullet points/nested enumerations are captured together or apart... but maybe that's for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surrounding_chars(txt, radius=1):\n",
    "    surrounding_chars = []\n",
    "    all_period_idx = [indices.start() for indices in re.finditer(\"\\.\", txt)]\n",
    "    \n",
    "    for period_idx in all_period_idx:\n",
    "        start_idx = period_idx - radius\n",
    "        end_idx = period_idx + radius + 1\n",
    "        substring = txt[start_idx: end_idx]\n",
    "        \n",
    "        if substring:\n",
    "            surrounding_chars.append(substring)\n",
    "    \n",
    "    return surrounding_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrounding_chars_1 = get_surrounding_chars(txt)\n",
    "surrounding_chars_2 = get_surrounding_chars(txt, radius=2)\n",
    "\n",
    "print(f\"For 1 character before and after a period, we have {len(set(surrounding_chars_1))} unique patterns\")\n",
    "print(f\"For 2 characters before and after a period, we have {len(set(surrounding_chars_2))} unique patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_chars(neighboring_chars):\n",
    "    possible_chars = defaultdict(list)\n",
    "\n",
    "    for pattern in neighboring_chars:\n",
    "        if pattern[-1] == \" \":\n",
    "            possible_chars[\" \"].append(pattern)\n",
    "        elif pattern[-1].isalpha():\n",
    "            possible_chars[\"alpha\"].append(pattern)\n",
    "        elif pattern[-1].isnumeric():\n",
    "            possible_chars[\"numeric\"].append(pattern)\n",
    "        elif not pattern[-1].isalnum():\n",
    "            possible_chars[\"symbol\"].append(pattern)\n",
    "        else:\n",
    "            possible_chars[\"other\"].append(pattern)\n",
    "    \n",
    "    print(f\"Total: {len(neighboring_chars)}\")\n",
    "    return possible_chars\n",
    "\n",
    "def print_char_stats(possible_chars):\n",
    "    print(f\"Space: {len(possible_chars[' '])}\"), \n",
    "    print(f\"Alpha: {len(possible_chars['alpha'])}\"), \n",
    "    print(f\"Numeric: {len(possible_chars['numeric'])}\"), \n",
    "    print(f\"Symbol: {len(possible_chars['symbol'])}\"), \n",
    "    print(f\"Other: {len(possible_chars['other'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_chars = get_possible_chars(surrounding_chars_1)\n",
    "print_char_stats(possible_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_chars = get_possible_chars(set(surrounding_chars_1))\n",
    "print_char_stats(possible_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wri-env",
   "language": "python",
   "name": "wri-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
